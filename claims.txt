\section{Proposed Preconditioning Setup}
\label{sec:learned_soc}

We solve SPD systems \(A x = b\) using PCG with an AMG preconditioner.  The only component we replace with learning is the construction of the \emph{strength-of-connection} structure: our GNN outputs a row-sparse binary strength matrix \(C\) (symmetric, with ones on the diagonal), and we pass it into Smoothed Aggregation AMG as a predefined strength input.  All downstream AMG steps (aggregation/coarsening, prolongation smoothing, and the multilevel V-cycle) remain standard.

\subsection{Features}
\label{subsec:features}

Our policy operates on the directed graph induced by the sparsity pattern of \(A\): there is a directed edge \((i,j)\) for each nonzero \(A_{ij}\) (equivalently, each CSR adjacency entry). The GNN produces edge logits that define a stochastic policy over outgoing edges per row, from which we sample a degree-controlled set of ``strong'' connections and assemble a symmetric binary strength matrix \(C\) passed into SA-AMG via \(\texttt{strength}=(\texttt{"predefined"},\{\texttt{"C"}:C\})\).

\subsubsection{Node features and node learning}
\label{subsec:node_features_learning}

\paragraph{Raw node features.}
Each node \(i\) is assigned a feature vector consisting of (i) four relaxed ``smooth'' test vectors and (ii) normalized geometric coordinates:
\begin{equation}
x_i \;=\; \bigl[r_i^{(1)},\, r_i^{(2)},\, r_i^{(3)},\, r_i^{(4)},\, x_i,\, y_i\bigr] \in \mathbb{R}^{6}.
\label{eq:node_feature_vector}
\end{equation}
The coordinate pair \((x_i,y_i)\in[0,1]^2\) is the normalized grid location used in our methodology.

\paragraph{Relaxed vectors (algebraically smooth error probes).}
The relaxed vectors \(\{r^{(k)}\}_{k=1}^4\) are generated by applying a few iterations of weighted Jacobi relaxation to the homogeneous system \(A e = 0\), starting from random initial errors. For each \(k\in\{1,\dots,4\}\),
\begin{align}
e^{(0)} &\sim \mathcal{N}(0,I), \\
e^{(t+1)} &= \bigl(I - \omega D^{-1}A\bigr)e^{(t)},\qquad t=0,\dots,T-1,
\label{eq:jacobi_relax}
\end{align}
with \(D=\mathrm{diag}(A)\), \(\omega=\tfrac{2}{3}\), 
(double check if this is Jacobi or Richardson)



and \(T=5\). We then normalize the resulting vectors by a global max-absolute scaling for stability. These relaxed vectors approximate \emph{algebraically smooth} error components (high-frequency components are damped by relaxation), which are precisely the components AMG must represent accurately on coarse levels.

\paragraph{Node embedding via message passing.}
Given the node features \(x_i\) and the directed sparsity graph, we learn node embeddings \(h_i\in\mathbb{R}^{d}\) using two Graph Attention (GAT) \cite{Velickovic2018GAT} layers:
\begin{equation}
h_i \;=\; \mathrm{GAT}_2\!\bigl(\sigma(\mathrm{GAT}_1(x_i, E)) , E\bigr),
\qquad h_i \in \mathbb{R}^{d},\ d=64,
\label{eq:gat_embeddings}
\end{equation}
where \(E\) denotes the directed edge set from \(\mathrm{supp}(A)\) and \(\sigma(\cdot)\) is an ELU nonlinearity. We chose graph attention because it realizes an adaptive, edge-wise weighting of neighbor
information: each node embedding is constructed as a data-dependent convex combination of
neighbor messages, enabling the network to learn which algebraic/geometric couplings should
dominate the representation used to score strong connections.

\paragraph{A basic property (convex aggregation).}
In a single-head GAT layer, the update has the form
\begin{equation}
h_i' \;=\; \sum_{j\in\mathcal{N}^+(i)} \alpha_{ij}\, W h_j,
\qquad
\alpha_{ij} \;=\; \frac{\exp(e_{ij})}{\sum_{k\in\mathcal{N}^+(i)} \exp(e_{ik})},
\label{eq:gat_update}
\end{equation}
where \(e_{ij}\in\mathbb{R}\) is a learned compatibility score and \(W\) is a learned linear map.
\begin{lemma}
For every node \(i\), the attention weights satisfy \(\alpha_{ij}\ge 0\) and
\(\sum_{j\in\mathcal{N}^+(i)} \alpha_{ij}=1\). Hence \(h_i'\) lies in the convex hull of
\(\{W h_j: j\in\mathcal{N}^+(i)\}\).
\end{lemma}
\begin{proof}
Because \(\exp(\cdot)>0\), we have \(\alpha_{ij}>0\) for all \(j\).
Moreover,
\[
\sum_{j\in\mathcal{N}^+(i)} \alpha_{ij}
=
\sum_{j\in\mathcal{N}^+(i)}
\frac{\exp(e_{ij})}{\sum_{k\in\mathcal{N}^+(i)} \exp(e_{ik})}
=
\frac{\sum_{j\in\mathcal{N}^+(i)} \exp(e_{ij})}{\sum_{k\in\mathcal{N}^+(i)} \exp(e_{ik})}
=1.
\]
Therefore \(h_i'=\sum_j \alpha_{ij}(W h_j)\) is a convex combination of the vectors \(W h_j\),
so \(h_i'\in \mathrm{conv}\{W h_j\}_{j\in\mathcal{N}^+(i)}\).
\end{proof}


% \paragraph{Optional learned near-nullspace candidates \(B\).}
% In addition to learning \(C\), we optionally learn additional near-nullspace candidate vectors for SA-AMG from the embeddings:
% \begin{equation}
% B_{\mathrm{extra}} \;=\; \mathrm{MLP}_B(h)\in\mathbb{R}^{n\times p},\qquad p=2,
% \end{equation}
% then forms
% \begin{equation}
% B \;=\; \bigl[\mathbf{1},\, \widehat{B}_{\mathrm{extra}}\bigr] \in \mathbb{R}^{n\times (1+p)},
% \label{eq:B_construction}
% \end{equation}
% where each column of \(B_{\mathrm{extra}}\) is mean-centered and \(\ell_2\)-normalized to improve numerical stability before concatenation with the constant vector \(\mathbf{1}\). This matches \texttt{build\_B\_for\_pyamg()}.

\subsubsection{Edge features and edge learning}
\label{subsec:edge_features_learning}

For each directed edge \((i,j)\), we build explicit edge descriptors and concatenate them with the learned node embeddings \((h_i,h_j)\) to produce an edge logit \(\ell_{ij}\) that parameterizes the policy over outgoing edges of node \(i\).

\paragraph{(1) Algebraic coupling magnitude.}
Let \(\tilde{A}_{ij}\) denote the edge weight normalized by the maximum absolute edge weight in the sample. We use
\begin{equation}
w_{ij} \;=\; |\tilde{A}_{ij}|.
\label{eq:edge_weight_feature}
\end{equation}

\paragraph{(2) Smoothness similarity from relaxed vectors.}
Using the first four channels of \(x_i\) (the relaxed-vector values), define
\begin{equation}
\mathrm{sim}_{ij}
\;=\;
\frac{\langle h_i, h_j\rangle}{\|h_i\|_2\,\|h_j\|_2 + \varepsilon},
% \qquad
% r_i = (r_i^{(1)},r_i^{(2)},r_i^{(3)},r_i^{(4)})\in\mathbb{R}^4,
\label{eq:cosine_sim}
\end{equation}
with \(\varepsilon>0\) small. This encodes the AMG intuition that strong connections often align with correlated smooth error.

\paragraph{(3) Direction-aware geometric features.}
Let \(p_i=(x_i,y_i)\) be the coordinate of node \(i\). Define displacement
\(
\delta_{ij} = p_i - p_j = (\Delta x_{ij}, \Delta y_{ij})
\)
and the unit direction
\(
(\hat{\Delta x}_{ij},\hat{\Delta y}_{ij})
=
(\Delta x_{ij},\Delta y_{ij})/\|\delta_{ij}\|_2
\).
The direction feature vector is exactly
\begin{equation}
d_{ij}
=
\bigl[
\Delta x_{ij},\, \Delta y_{ij},\,
|\Delta x_{ij}|,\, |\Delta y_{ij}|,\,
\hat{\Delta x}_{ij},\, \hat{\Delta y}_{ij}
\bigr]\in\mathbb{R}^6.
\label{eq:dir_features}
\end{equation}
(Note: \(\|\delta_{ij}\|_2\) is used only for normalization; it is not concatenated as its own feature in the current implementation.)

\paragraph{Edge logit model.}
We concatenate learned embeddings and explicit edge descriptors into one edge feature vector



\begin{equation}
\phi_{ij}
=
\bigl[
h_i,\ h_j,\ w_{ij},\ \mathrm{sim}_{ij},\ d_{ij}
\bigr],
\label{eq:edge_feature_vector}
\end{equation}
and compute an edge logit with an MLP $f_\theta$:
\begin{equation}
\ell_{ij} \;=\; f_{\theta}(\phi_{ij}) \in \mathbb{R}.
\label{eq:edge_logit}
\end{equation}
Diagonal edges \((i,i)\) are masked by setting \(\ell_{ii}\leftarrow -\infty\) (implemented as a large negative constant), ensuring the policy never selects self-edges.

\subsubsection{Final output: from sampled edges to a strength matrix \(C\)}
\label{subsec:final_output_C}

\paragraph{Row-wise degree-controlled sampling.}
For each node \(i\), let \(\mathcal{N}^+(i)\) denote its outgoing neighbors (edges in row \(i\)). We sample up to \(k\) edges from \(\mathcal{N}^+(i)\) \emph{without replacement} using a sequential categorical policy with temperature \(\tau\) (and a Gumbel perturbation for exploration). Conceptually, this produces a binary directed mask \(m_{ij}\in\{0,1\}\) with at most \(k\) ones per row:
\begin{equation}
\sum_{j\in\mathcal{N}^+(i)} m_{ij} \;\le\; k.
\end{equation}

\paragraph{Symmetrization and diagonal completion.}
From the sampled directed mask, we build a sparse binary matrix \(C\) by placing ones on selected edges, symmetrizing, and adding the identity:
\begin{equation}
C \;=\; \max(C_{\mathrm{dir}}, C_{\mathrm{dir}}^{\top}) \;+\; I,
\label{eq:C_build}
\end{equation}
so \(C\) is symmetric with ones on the diagonal. 

\paragraph{AMG consumption.}
Finally, SA-AMG is constructed using this learned strength matrix via the predefined-strength interface:
\[
\texttt{ml} \leftarrow \texttt{pyamg.smoothed\_aggregation\_solver}(A,\ \texttt{strength}=(\texttt{"predefined"},\{\texttt{"C"}:C\}),\ \texttt{B}=B),
\]
\subsection{Training}
\label{subsec:training}

\paragraph{Unsupervised (no labels) black-box learning objective.}
Our goal is to learn a strength-of-connection matrix \(C\) that makes the resulting SA-AMG hierarchy
a stronger \emph{preconditioner} (i.e., yields a faster contraction factor per multigrid cycle and thus fewer PCG iterations).
There is no supervised target \(C^\star\) available; instead, we treat PyAMG as an in-the-loop black box and define
a scalar performance signal directly from the \emph{measured} error reduction of a single multigrid V-cycle.
This yields an unsupervised reinforcement-learning formulation: the GNN induces a stochastic policy over edges, we sample \(C\),
build an AMG solver with \(\texttt{strength}=(\texttt{"predefined"},\{\texttt{"C"}:C\})\), and score it via V-cycle efficiency.

\paragraph{Policy and action space}
Fix our SPD matrix \(A\) and let \(E=\{(i,j):A_{ij}\neq 0\}\) be its directed sparsity graph. The policy network (parameters \(\theta\))
assigns a real-valued logit \(\ell_{ij}\) to every directed edge \((i,j)\in E\).
The \emph{action} of the policy is the complete set of selected directed edges across all rows:
\begin{equation}
\mathcal{S}
\;=\;
\bigl\{(i,j_{i,1}), (i,j_{i,2}), \dots, (i,j_{i,k_i})\bigr\}_{i=1}^n,
\label{eq:action_S_def}
\end{equation}
i.e., for each row/node \(i\) we select \(k_i\) outgoing neighbors (at most \(k\), depending on degree).
The policy \(\pi_\theta\) is therefore a probability distribution over such global edge-selection outcomes \(\mathcal{S}\).
In words, \(\pi_\theta(\mathcal{S})\) is the probability that the stochastic sampling procedure (defined below) selects
\emph{exactly} the directed edge set \(\mathcal{S}\) when run on the current logits.

\paragraph{Policy: row-wise edge selection without replacement.}
For each row \(i\), let \(\mathcal{N}^+(i)=\{j:(i,j)\in E\}\) denote the outgoing neighbors.
We select up to \(k\) outgoing edges from \(\mathcal{N}^+(i)\) \emph{sequentially without replacement}.
At selection step \(t\in\{1,\dots,k_i\}\) within row \(i\), define the set of \emph{remaining} candidates
\begin{equation}
\mathcal{R}_{i,t}
\;:=\;
\mathcal{N}^+(i)\setminus \{j_{i,1},\dots,j_{i,t-1}\},
\label{eq:remaining_set}
\end{equation}
i.e., we remove neighbors already chosen in earlier steps.
Given \(\mathcal{R}_{i,t}\), we form a categorical distribution over the remaining candidates with temperature \(\tau>0\):
\begin{equation}
\pi_\theta\bigl((i,j)\mid i,t,\mathcal{R}_{i,t}\bigr)
\;=\;
\frac{\exp(\ell_{ij}/\tau)}{\sum\limits_{(i,k)\in\mathcal{R}_{i,t}} \exp(\ell_{ik}/\tau)},
\qquad (i,j)\in\mathcal{R}_{i,t}.
\label{eq:policy_row_softmax}
\end{equation}
We then sample a single neighbor \(j_{i,t}\) from \eqref{eq:policy_row_softmax}
(implemented in \texttt{seven.py} via a Gumbel-perturbed argmax for exploration, while still accumulating
the corresponding log-probability under \eqref{eq:policy_row_softmax}).

\paragraph{Probability of an entire sampled action \(\pi_\theta(\mathcal{S})\).}
Because the procedure samples sequentially (and without replacement), the probability of the \emph{entire}
selected edge set \(\mathcal{S}\) factorizes as a product of the stepwise conditional probabilities:
\begin{equation}
\pi_\theta(\mathcal{S})
\;=\;
\prod_{i=1}^{n}\ \prod_{t=1}^{k_i}
\pi_\theta\bigl((i,j_{i,t})\mid i,t,\mathcal{R}_{i,t}\bigr).
\label{eq:pi_of_S_product}
\end{equation}
Equivalently, the log-probability accumulated in code (\texttt{logp\_sum}) is
\begin{equation}
\log \pi_\theta(\mathcal{S})
\;=\;
\sum_{i=1}^{n}\ \sum_{t=1}^{k_i}
\log \pi_\theta\bigl((i,j_{i,t})\mid i,t,\mathcal{R}_{i,t}\bigr).
\label{eq:logprob_sum}
\end{equation}
This quantity is crucial: although PyAMG and the discrete sampling are not differentiable,
\(\log \pi_\theta(\mathcal{S})\) \emph{is} differentiable with respect to \(\theta\), and REINFORCE uses it to push probability mass
toward selections \(\mathcal{S}\) that yield higher reward.

\paragraph{From \(\mathcal{S}\) to the strength matrix \(C\).}
The sampled directed edges \(\mathcal{S}\) define a directed binary mask \(C_{\mathrm{dir}}\) with ones on selected edges.
We then symmetrize and add the identity (as in \texttt{C\_from\_selected\_edges()}):
\begin{equation}
C \;=\; \max(C_{\mathrm{dir}}, C_{\mathrm{dir}}^\top) + I.
\label{eq:C_sym}
\end{equation}

\paragraph{Reward = measured residual/error decay (plus complexity control).}
To measure preconditioner quality, we evaluate a single AMG V-cycle as an approximate inverse on the homogeneous system \(Ax=0\).
We first generate a batch of \emph{smooth} error vectors \(\{e_0^{(m)}\}_{m=1}^M\) by applying \(T_{\mathrm{test}}\) steps of weighted Jacobi
to random initial errors (cf.\ \texttt{relaxed\_smooth\_vectors} with \texttt{relax\_iters=25}).
For each smooth error \(e_0^{(m)}\), we apply one multigrid cycle \(B\) (implemented by \texttt{ml.solve} with \texttt{maxiter=1})
to obtain \(e_1^{(m)}\), and compute the $A$-energy reduction ratio \cite{BriggsHensonMcCormick2000MultigridTutorial2e}
\begin{equation}
\rho^{(m)} \;=\;
\frac{\lVert e_1^{(m)} \rVert_A^2}{\lVert e_0^{(m)} \rVert_A^2},
\qquad
\lVert e \rVert_A^2 := e^\top A e.
\label{eq:energy_ratio}
\end{equation}
We convert these ratios into a scalar reward by averaging the log-contraction:
\[
R_{\mathrm{base}} : \mathcal{S} \rightarrow \mathbb{R}
\]
\begin{equation}
R_{\mathrm{base}}
\;=\;
-\frac{1}{M}\sum_{m=1}^M \log(\rho^{(m)}),
\label{eq:base_reward}
\end{equation}
so that larger reward corresponds to stronger contraction per V-cycle (smaller \(\rho^{(m)}\)).

To prevent degenerate solutions that improve contraction by making coarse operators overly dense,
we penalize operator complexity:
\[
\mathrm{opC}:\mathcal{S} \rightarrow \mathbb{R}
\]
\begin{equation}
\mathrm{opC}
\;=\;
\frac{\sum_{\ell=0}^{L-1}\mathrm{nnz}(A_\ell)}{\mathrm{nnz}(A_0)},
\qquad
\mathrm{penalty}
\;=\;
\lambda\,\max\{0,\mathrm{opC}-\tau_{\mathrm{opC}}\},
\label{eq:op_complexity}
\end{equation}
and define the final reward
\begin{equation}
R \;=\; R_{\mathrm{base}} - \mathrm{penalty},
\label{eq:final_reward}
\end{equation}
with \(\tau_{\mathrm{opC}}=1.35\) and \(\lambda=1.0\) in our default configuration.

\paragraph{REINFORCE policy gradient}
Because PyAMG is not differentiable end-to-end, we use a Monte Carlo policy-gradient estimator, the so-called REINFORCE algorithm \cite{Williams1992REINFORCE} in reinforcement learning.
The objective is to maximize expected reward
\(
J(\theta)=\mathbb{E}_{\mathcal{S}\sim\pi_\theta}[R(\mathcal{S})]
\),
whose gradient satisfies
\begin{equation}
\nabla_\theta J(\theta)
\;=\;
\mathbb{E}_{\mathcal{S}\sim\pi_\theta}\!\left[
R(\mathcal{S})\,\nabla_\theta \log \pi_\theta(\mathcal{S})
\right].
\label{eq:reinforce}
\end{equation}
Thus, \(\nabla_\theta \log \pi_\theta(\mathcal{S})\) is the mechanism by which parameters \(\theta\) are updated to increase the probability
of sampled edge sets \(\mathcal{S}\) that achieve higher measured V-cycle contraction.

To reduce variance, we use a moving-average baseline \(b\) (cf.\ \texttt{baseline\_momentum=0.95}),
\begin{equation}
b \leftarrow \beta b + (1-\beta)R,
\qquad
A := R-b,
\label{eq:baseline_advantage}
\end{equation}
and minimize the stochastic loss
\begin{equation}
\mathcal{L}(\theta)
\;=\;
- A \cdot \log \pi_\theta(\mathcal{S}),
\label{eq:reinforce_loss}
\end{equation}
which matches \texttt{train\_policy} in \texttt{seven.py}:
\(\texttt{loss = -(advantage * logp\_sum)}\).
We optimize \(\mathcal{L}\) with Adam (\(\texttt{lr}=2\times 10^{-3}\)) and apply gradient clipping (\(\|\nabla\|_2\le 1\)).

\paragraph{Practical details and logging.}
Training samples are generated from a small distribution of diffusion operators (isotropic, axis-aligned anisotropic,
and random rotated anisotropy) so the policy must generalize across \((\varepsilon,\theta)\).
We anneal the sampling temperature \(\tau\) mildly each epoch (\(\tau\leftarrow \max(0.5,0.97\,\tau)\)) to transition from exploration to exploitation.
We log reward, baseline, advantage, and loss to TensorBoard, along with a physics-motivated probe that checks whether the learned logits
separate strong horizontal couplings from weak vertical couplings on a fixed anisotropic grid (\texttt{probe/discrimination} in \texttt{seven.py}).

