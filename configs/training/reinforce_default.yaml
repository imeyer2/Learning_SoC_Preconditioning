# Training Configuration

# RL Algorithm: reinforce, ppo, a2c (currently only reinforce implemented)
algorithm: reinforce

# Training parameters
epochs: 40
batch_size: 1  # Process one problem at a time (policy gradient per problem)

# Optimizer
optimizer:
  type: adam
  lr: 0.002
  weight_decay: 0.0
  grad_clip: 1.0

# Learning rate scheduler (optional)
scheduler:
  enabled: false
  type: step  # step, cosine, exponential
  step_size: 10
  gamma: 0.9

# Temperature for stochastic sampling
temperature:
  initial: 0.9
  anneal: true
  anneal_factor: 0.97
  min_temperature: 0.5

# Sampling strategy
sampling:
  # Learnable k: If true, the model predicts per-node k values
  # If false, uses fixed k_per_row for all nodes
  learnable_k: true
  
  k_per_row: 5  # Number of strong edges to select per node (used when learnable_k=false)
  k_min: 1      # Minimum k when learnable (prevents degenerate solutions)
  k_max: 25      # Maximum k when learnable (prevents too dense C)
  
  method: topk_without_replacement  # topk_without_replacement, gumbel_softmax
  gumbel: true

# Reward configuration
reward:
  function: pcg_residual_reduction  # vcycle_energy_reduction, pcg_iterations, hybrid
  
  # V-cycle reward parameters
  vcycle:
    num_test_vecs: 6
    relax_iters: 25
    omega: 0.6666666  # 2/3
    cycle_type: V  # V or W
  
  # PCG residual reduction reward parameters (if function: pcg_residual_reduction)
  pcg:
    num_iters: 5  # Number of PCG iterations to run
    num_test_vecs: 3  # Number of random RHS to average over
  
  # Complexity penalty
  complexity:
    target: 1.35
    penalty_weight: 1.0
  
  # Hybrid reward (if function: hybrid)
  hybrid:
    vcycle_weight: 0.7
    pcg_weight: 0.3

# Baseline for variance reduction
baseline:
  type: exponential_moving  # moving_average, exponential_moving
  momentum: 0.95  # For mean EMA
  var_momentum: 0.99  # For variance EMA (exponential_moving only)
  normalize: true  # Normalize advantages by std (exponential_moving only)
  clip: 10.0  # Clip normalized advantages (exponential_moving only)
  warmup_steps: 100  # Use simple stats during warmup (exponential_moving only)
  
  # Value network baseline (if type: value_network) - not yet implemented
  value_network:
    hidden_dim: 64
    lr: 0.001

# Entropy coefficient for exploration bonus
# Higher = more exploration, Lower = more exploitation
# Recommended: 0.01-0.1
entropy_coef: 0.01

# Parallel reward computation
# Speeds up training by computing PyAMG rewards across multiple CPU cores
# Recommended: Set to number of CPU cores available (e.g., 8-16 on HPC)
parallel_workers: 1  # Number of parallel workers (1 = sequential)
parallel_batch_size: 8  # Batch size for parallel training

# Logging and checkpointing
logging:
  log_interval: 1  # Log every N steps
  probe_interval: 1  # Run probe diagnostics every N epochs
  save_interval: 5  # Save checkpoint every N epochs

# Experiment tracking
experiment:
  name: null  # Auto-generated if null
  output_dir: outputs
  save_best: true
  save_last: true

# PyAMG solver settings
pyamg:
  coarse_solver: splu  # splu, lu, cg, gauss_seidel
  max_coarse: 50  # Maximum DOFs on coarsest level (larger = faster setup, slower V-cycle)
  # ⚠️ If max_coarse is too large, direct solve becomes expensive
  # For small training grids (16x16=256 DOFs), max_coarse=50 is fine
  # For larger grids, keep max_coarse small (10-50)
  
# Device
device: cpu  # cpu or cuda
